{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.set_start_method(\"spawn\")\n",
    "\n",
    "import os\n",
    "from threading import Thread\n",
    "from typing import Iterator\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MAX_NEW_TOKENS = 2048\n",
    "DEFAULT_MAX_NEW_TOKENS = 1024\n",
    "\n",
    "MAX_INPUT_TOKEN_LENGTH = int(os.getenv(\"MAX_INPUT_TOKEN_LENGTH\", \"4096\"))\n",
    "\n",
    "model_id = \"deepseek-ai/deepseek-coder-33b-instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.use_default_system_prompt = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    message: str,\n",
    "    chat_history: list,\n",
    "    system_prompt: str,\n",
    "    max_new_tokens: int = 1024,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50,\n",
    "    repetition_penalty: float = 1.5,\n",
    ") -> Iterator[str]:\n",
    "    conversation = []\n",
    "    if system_prompt:\n",
    "        conversation.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "    for user, assistant in chat_history:\n",
    "        conversation.extend([{\"role\": \"user\", \"content\": user}, {\"role\": \"assistant\", \"content\": assistant}])\n",
    "    conversation.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(conversation, return_tensors=\"pt\")\n",
    "    if input_ids.shape[1] > MAX_INPUT_TOKEN_LENGTH:\n",
    "        input_ids = input_ids[:, -MAX_INPUT_TOKEN_LENGTH:]\n",
    "        print(f\"Trimmed input from conversation as it was longer than {MAX_INPUT_TOKEN_LENGTH} tokens.\")\n",
    "\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False,\n",
    "        num_beams=1,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        eos_token_id=32021\n",
    "    )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    outputs = []\n",
    "    for text in streamer:\n",
    "        outputs.append(text)\n",
    "        yield \"\".join(outputs).replace(\"\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get user input for prompts\n",
    "        message = input(\"User: \")\n",
    "        #system_prompt = input(\"System prompt (press Enter if none): \")\n",
    "\n",
    "        # Set an empty chat history initially\n",
    "        chat_history = []\n",
    "\n",
    "        # Generate response\n",
    "        for response in generate(message, chat_history):\n",
    "            print(\"Bibby Powers Florida USA: \", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
